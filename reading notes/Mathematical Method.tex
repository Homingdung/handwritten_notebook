\documentclass{article}

\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,hyperref}

\newcommand{\R}{\mathbf{R}}  
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Q}{\mathbf{Q}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{question}{Question}[section]
\newtheorem{problem}{Problem}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}



\begin{document}

% ------------------------------------------ %
%                 START HERE                  %
% ------------------------------------------ %

\title{Mathematical Method} % Replace with appropriate title
\author{Mingdong He\\The University of Nottingham} 

\maketitle

\tableofcontents
% -----------------------------------------------------
% The following two environments (theorem, proof) are
% where you will enter the statement and proof of your
% first problem for this assignment.
%
% In the theorem environment, you can replace the word
% "theorem" in the \begin and \end commands with
% "exercise", "problem", "lemma", etc., depending on
% what you are submitting. 
% -----------------------------------------------------
\section{Introduction}
This is my reading notes based on \textit{Mathematical Methods,  University of Cambridge Part IB Mathematical Tripos,  David Skinner}.  This notes is largely imcomplete one, I only write down some essential parts based on my own learning path.

\section{Fourier Series}.
\begin{definition}[Innner product]
Inner product is defined as a mapping $(,):V \times V \to F$ that obeys:
\begin{enumerate}
	\item $(u,v)=(v,u)*$ (Conjugate symmetry)
	\item $(u,\lambda v)=\lambda(u,v)$ (linearity)
	\item $(u,v+w)=(u,v)+(u,w)$ (additivity)
	\item $(u,u) \geq 0, \forall u \in V$, and with equality $\iff u=0$
\end{enumerate}
\end{definition}

If $F=\mathbb{C}$, the map is called sesquilinear.


\subsection{Motivation}
If we're given an orthonormal basis, we can use the inner product to explicitly decompose a general into. this basis. 

For any element $u\in V$,  it can be uniquely written as 
\begin{equation}
u=\sum_{i=1}^n \lambda_iv_i
\end{equation}

we can use inner product to find the coefficient $\lambda_i$. Please note that $(v_i,v_j)=\delta_{ij}$.
\begin{equation}
(v_j,u)=(v_j,\sum_{i=1}^n\lambda_i v_i)=\sum_{i=1}^n (v_i,\lambda_i v_i)=\sum_{i=1}^n \lambda_i(v_j,v_i)=\lambda_j
\end{equation}

Thus, we find that $\lambda_i=(v_i,u)$, for real vectors, $\lambda_j$ is just the projection of $u$ onto $v_j$.

\begin{definition}[Inner product for complex valued function]
Consider a function $f: \Omega \to \mathbb{C}$. The inner product is defined as:
\begin{equation}
(f,g)=\int_{\Omega}f(x)^*g(x)d\mu
\end{equation}
\end{definition}
This is an generalization of the inner product between two finite dimensional vectors. 

If $\Omega = [a,b]$, then $(f,g)=\int_a^b f(x)^*g(x)dx$, with measure $dx$
If $\Omega$ is disc $D_2$, then $(f,g)=\int_{r=0}^1\int_{\theta=0}^{2\pi}f(r,\theta)^*g(r,\theta)rdrd\theta$, with. measure $d\mu=rdrd\theta$.

Let's applied above results to construct fourier series.

Consider inner product:
\begin{equation}
(e^{im\theta},e^{in\theta})=\int_{\pi}^{\pi}e^{-im\theta}e^{in\theta}d\theta=2\pi \delta_{m,n}
\end{equation}

this function $e^{in\theta}$ can be used as orthonormal basis after some scaling.

Fourier claimed that 
\begin{equation}
f(\theta)\stackrel{?}{=}\sum_{n \in \mathbb{Z}}\hat{f_n}e^{in\theta}
\end{equation}


Since our basis is orthonormal, we can use inner product to get the cofficients $\hat{f_n}$.
\begin{equation}
\hat{f_n}=\frac{1}{2\pi}(e^{in\theta},f)=\frac{1}{2\pi}\int_{-\pi}^{\pi}e^{-in\theta}f(\theta)d\theta
\end{equation}


Let's rewrite the function:

\begin{align}
f&=\sum_{n\in \mathbb{Z}}\hat{f_n}e^{in\theta}\\
&=\hat{f_0}+\sum_{n=1}^{\infty}(\hat{f_n}e^{in\theta}+\hat{f}_{-n}e^{-in\theta})\\
&=\hat{f_0}+\sum_{n=1}^{\infty}(\hat{f_n}e^{in\theta}+\hat{f}_{n}^{*}e^{-in\theta})\\
&=\hat{f_0}+\sum_{n=1}^{\infty}a_n\cos{n\theta}+b_n\sin{n\theta}
\end{align}

In the final line, we set $\hat{f_n}=(a_n-ib_n)/2$, then we get
\begin{equation}
a=2 \Re \hat{f_n},b=-2\Im \hat{f_n}
\end{equation}
\begin{equation}
a=\frac{1}{\pi}\int_{-\pi}^{\pi}\cos{n\theta}f(\theta)d\theta, b=\frac{1}{\pi}\int_{-\pi}^{\pi}\sin{n\theta}f(\theta)d\theta
\end{equation}


\subsection{Some results}

\begin{definition}
The finite sum is defined as 
\begin{equation}
S_nf=\sum_{k=-n}^{n}\hat{f_k}e^{in\theta}
\end{equation}
\end{definition}


\begin{theorem}[Parseval's identity]
\begin{equation}
(f,f)=2\pi \sum_{n\in \mathbb{Z}} |\hat{f_n}|^2
\end{equation}
\end{theorem}

Parseval's indentity can be view as an infinite dimensional version of Phythagoras'theorem: the norm squared $f$ is equal to the sum of the (mod)-squares of its coefficients in the (Fourier) basis of orthogonal functions. If we view the Fourier series as a map from a function to a sequence, Parseval's identity tells us this map is an isometry, which means it preserves lengths.

\section{Sturm-Liouville Theory}
\subsection{Matices foundation: Self-adjoint matrices}
\begin{definition}[Linear Map]
Let $V$ and $W$ be finite dimensional vector spaces (over complex numbers), with $\dim{V}=n,\dim{W}=m$, a linear map is defined by $M:V\to M$
\end{definition}

We can. represent the map $M$ in terms of an $m\times n $ matrix $M$, with components:
\begin{equation}
M_{ai}=(w_a,Mv_i), a=1,\ldots,m, i=1,\ldots,n
\end{equation}

\begin{definition}[Hermitian conjugate]
Given a matirx $M$, its Hermitian conjugate $M^{\dag}$ is defined to be the complex conjugate of the transpose matrix, $M^{\dag}=(M^{T})^{*}$, where the complex conjugation acts on each entry of $M^{T}$.
\end{definition}

A matrix is said to be Hermitian or self-adjoint if $M^{\dag}=M$. This is also another neaty way of this definition. since for two vectors we have, $(u,v)=u^{\dag} \cdot v$, (if both vector are real, this is same as inner product as usual, also, $(Bu)^{\dag}=u^{\dag}B^{\dag}$, we have the definition that: matrix $B$ is the adjoint of a matrix $A$ $\iff$ $(Bu,v)=(u,Av)$. 

There some important properties below.

\begin{enumerate}
	\item Since $\lambda_i(v_i,v_i)=(v_i,Mv_i)=(Mv_i,v_i)=\lambda_i^{*}(v_i,v_i)$, the eigenvalues of self-adjoint matrix are always real (because $\lambda_i=\lambda_i^{*}$).
	\item We have $\lambda_i(v_j,v_i)=(v_j,Mv_i)=(Mv_j,v_i)=\lambda_i(v_i,v_i)$, then $(\lambda_i-\lambda_j)(v_j,v_i)=0$, the eigenvectors corresponding to distinct eigenvalues are orthogonal w.r.t. the inner product.
\end{enumerate}

\subsection{Solving linear system}

A self-adjoint matrix $M$ is non-singular $\iff$ all its eigenvalues are non-zero. In this case, we can solve the linear equation $\mathbf{Mu}=\mathbf{f}$. The solution is $\mathbf{u}=\mathbf{M^{-1}f}$. 

Suppose $\{v_1,v_2,\ldots,v_n\}$ is an orthonormal basis of eigenvectors of $\mathbf{M}$. Then we can write:
\begin{align}
\mathbf{f}=\sum_{i=1}^{n}f_i\mathbf{v_i},\\
\mathbf{u}=\sum_{i=1}^{n}u_i\mathbf{v_i}
\end{align}

where $f_i=(\mathbf{v_i},\mathbf{f})$ (see previous inner product). In order to solve $\mathbf{u}$, we need to find the coefficients $u_i$ in the $\{v_i\}$ basis.


By linearity:
\begin{equation}
\mathbf{Mu}=\sum_{i=1}^{n}u_i \mathbf{Mv_i}=\sum_{i=1}^{n}u_i\lambda_i\mathbf{v_i}=\mathbf{f}=\sum_{i=1}^n f_i\mathbf{v_i}
\end{equation}


Take the inner product of this equation with $\mathbf{v_j}$ gives

\begin{equation}
\sum_{i=1}^n u_i\lambda_i(v_j,v_i)=u_j\lambda_j=\sum_{i=1}^nf_i(v_j,v_i)=\sum_{i=1}^n f_i\delta_{ij}=f_j
\end{equation}

Thus,
\begin{equation}
\mathbf{u}=\sum_{i=1}^n\frac{f_i}{\lambda_i}\mathbf{v_i}
\end{equation}


If $\mathbf{M}$ is singular then either $\mathbf{Mu=f}$ has no solution or else has a non-unique solution.


\subsection{Differential Operator}

This section is highly related to the previous discussion about self-adjoint matrices.

\begin{definition}[Linear Operator]
\begin{equation}
L=A_p(x)\frac{d^p}{dx^p}+A_{p-1}\frac{d^{p-1}}{dx^{p-1}} + \ldots + A_1(x)\frac{d}{dx}+A_0(x)
\end{equation}
\end{definition}

This is a linear map between spaces of functions beacause for two function $y_{1,2}(x)$ and constants $c_{1,2}$, we have $L(c_1y_1+c_2y_2)=c_1Ly_1+c_2Ly_2$.

We will be interested in second order linear differential operators
\begin{equation}
L=P(x)\frac{d^2}{dx^2}+R(x)\frac{d}{dx}-Q(x)
\end{equation}

The homogenous equation $Ly(x)=0$ has precisely two non-trivial linearly independent solutions, say $y=y_1(x),y=y_2(x)$, and the general solution is $y(x)=c_1y_1(x)+c_2y_2(x)$, which is called complementary function. For inhomogenous equation $Ly(x)=f$, we seek any single solution $y(x)=y_p(x)$, which is called partial integral, and the final general solution is the a linear combination 
\begin{equation}
y(x)=c_py_p(x)+c_1y_1(x)+c_2y_2(x)
\end{equation}
Sturm-Liouville theory provides a more systematic approach, analogous to solving the matrix equation $\mathbf{Mu=f}$.


\subsection{Self-adjoint differential operators}
The second order differential operator considered by Sturm and Liouville take the form
\begin{equation}
Ly=\frac{d}{dx}\left(p(x)\frac{dy}{dx}\right)-q(x)y
\end{equation}

Provided $P(x)\neq 0$, divided through by $P(x)$ to obtain
\begin{equation}
\frac{d^2}{dx^2}+\frac{R(x)}{P(x)}\frac{d}{dx}-\frac{Q(x)}{P(x)}=e^{-\int_0^x R(t)/P(t)dt}\frac{d}{dx}\left(e^{\int_0^x R(t)/P(t)dt}\frac{d}{dx}\right)-\frac{Q(x)}{P(x)}
\end{equation}

with integrating factor $e^{\int_0^x R(t)/P(t)dt}$. 


The beautiful feature of Sturm-Liouville operators is that they are self-adjoint w.r.t inner product 
\begin{equation}
(f,g)=\int_a^bf(x)^{*}g(x)dx
\end{equation}


provided the functions on which they act obey appropriate boundary conditions. Let' s illustrate this fact.

Recall the neaty definition: matrx $B$ is the adjoint of a matrix $A$ $\iff$ $(Bu,v)=(u,Av)$. Thus, let take inner product $(Lf,g)$

\begin{align}
(Lf,g)&=\int_a^b\left[\frac{d}{dx}(p(x)\frac{df^*}{dx})-q(x)f^*(x)\right]g(x)dx\\
	  &=\left[p\frac{df^*}{dx}g\right]_a^b-\int_a^b p(x)\frac{df^*}{dx}\frac{dg}{dx}-q(x)f(x)^*g(x)dx\\
	  &=\left[p\frac{df^*}{dx}g-pf^*\frac{dg}{dx}\right]_a^b+\int_a^bf(x)^*\left[\frac{d}{dx}\left(p(x)\frac{dg}{dx}\right)-q(x)g(x)\right]dx\\
	  &=\underbrace{\left[p(x)\left(\frac{df^*}{dx}g-f^*\frac{dg}{dx}\right)\right]_a^b}_{boundary \ condition \ applied =0}+(f,Lg)
\end{align}


Exampls of such boundary conditions are to require that all our functions satisfy
\begin{align}
b_1f'(a)+b_2f(a)=0\\
c_1f'(b)+c_2f(b)=0
\end{align}


There are some cases about this boundary conditions
\begin{enumerate}
	\item If the function $p(x)$ obeys $p(a)=p(b)$, then all our functions are periodic, so that $f(a)=f(b), f'(a)=f'(b)$
	\item If $p(a)=p(b)=0$, the endpoints of the interval $[a,b]$ are singular points of the differential equation.
\end{enumerate}

This section need to be illustrated later.


\subsection{Eigenfunctions and weight functions}



\begin{definition}[Eigenfunction]
A function $y(x)$ is said to be an eigenfunction of $L$ with eigenvalue $\lambda$ and weight $w(x)$ if
\begin{equation}
Ly(x)=\lambda w(x)y(x)
\end{equation}
\end{definition}


\begin{definition}[Inner product with weight]
\begin{equation}
(f,g)_{\omega}=\int_a^b f(x)^*g(x)\omega(x)dx
\end{equation}
\end{definition}

Since omega(x) is real, so $(f,g)_\omega=(f,\omega g)=(\omega f,g)$


\begin{definition}[Eigenvalues of SL operator are always real]
If $Lf=\lambda \omega f$, then 
\begin{equation}
\lambda (f,f)_\omega =(f,\lambda \omega f)=(f,Lf)=(Lf,f)=\lambda^*(f,f)_\omega 
\end{equation}
\end{definition}

\begin{theorem}
Eigenfunctions $f_1,f_2$ with distinct eigenvalues, but the same weight function, are orthogonal w.r.t the inner product with weight $\omega$.
\end{theorem}
\begin{proof}
Since:
\begin{equation}
\lambda_i(f_j,f_i)_\omega=(f_j,Lf_i)=(Lf_j,f_i)=\lambda_j(f_j,f_i)_\omega
\end{equation}
so that if $\lambda_i \neq \lambda_j$,then:
\begin{equation}
(f_j,f_i)_\omega=\int_a^b f_j(x)^*f_i(x)\omega(x)dx=0
\end{equation}
\end{proof}


Application of this theorem is that: given a self-adjoint operator $L$, we can form an orthonormal set $\{Y_1(x),Y_2(x),\ldots \}$ of its eigenfunctions by setting:

\begin{equation}
Y_n(x)=y_n(x)/\sqrt{\int_a^b |y_n|^2\omega dx}
\end{equation}

Finally, making particular choice of boundary conditions, any function $f(x)$ in $[a,b]$ that obeys the chosen boundary conditions may be expanded as:
\begin{equation}
f(x)=\sum_{n=1}^\infty f_n Y_n(x), \ where \ f_n=(Y_n,f)_\omega =\int_a^b Y_n^*(x)f(x)\omega (x)dx
\end{equation}

Let's take a look at some example:
Taking the domain $[-L,L]$ and impose the homogeneous boundary conditions that all our functions are periodic i.e. $f(-L)=f(L),f'(-L)=f'(L)$. Choosing $p(x)=1$ and $q(x)=0$, thus SL operator is
\begin{equation}
L=\frac{d^2}{dx^2}
\end{equation}

Choosing weight function to be $1$:
The eigenfunction equation becomes:
\begin{equation}
Ly(x)=-\lambda y(x)
\end{equation}

If $\lambda <0$, the only solution that obeys the periodic boundary conditions is the trivial case $y(x)=0$; If $\lambda \geq 0$, then a basis of solution is given by

\begin{equation}
y_n(x)=\exp(i \frac{n \pi x}{L}), for \ \lambda_n=\left(\frac{n\pi}{L}\right)^2
\end{equation}

For another example:
\begin{equation}
\frac{1}{2}H''-xH'=-\lambda H(x)
\end{equation}

subject to the condition that $H(x)$ behaves like a polynomial as $|x| \to \infty$. This equation is not yet in SL form, so we first compute the integrating factor:
\begin{equation}
\frac{d}{dx}\left(e^{-x^2}\frac{dH}{dx}\right)=-2\lambda e^{-x^2}H(x)
\end{equation}


This equation is known as Hermite's equation. The solution is,

\begin{equation}
H_n(x)=(-1)^ne^{x^2}\frac{d^n}{dx^n}e^{-x^2}
\end{equation}

Thus, $H_0(x)=1, H_1(x)=2x, H_2(x)=4x^2-2, H_3(x)=8x^3-12x$, which is known as Hermite polynomial with the following property:
\begin{equation}
(H_m,H_n)_{e^{-x^2}}=\int_{-\infty}^{\infty}H_m(x)H_n(x)e^{-x^2}dx=\delta_{m,n}2^n\sqrt{\pi}n!
\end{equation}

\end{document}